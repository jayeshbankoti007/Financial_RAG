# üìä Financial RAG Q&A System

> **Advanced Retrieval-Augmented Generation system for financial document analysis powered by state-of-the-art embeddings and GPT-4o-mini**

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/streamlit-1.28+-red.svg)](https://streamlit.io)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Open Source](https://img.shields.io/badge/Open%20Source-üíö-brightgreen.svg)](https://github.com)

A production-ready RAG (Retrieval-Augmented Generation) system specifically designed for financial document analysis. Upload PDFs, ask natural language questions, and get accurate answers with precise source attribution.

![Demo Screenshot](https://via.placeholder.com/800x400/2E86AB/FFFFFF?text=Financial+RAG+Demo+Screenshot)

---

## üöÄ **Key Features**

### üß† **Advanced AI Architecture**
- **Multiple Embedding Models**: Support for BAAI BGE, E5, and multilingual models
- **Smart Query Optimization**: AI-powered query rewriting for better retrieval
- **Hybrid Search**: Dense vector search with FAISS optimization using HNSW for fast approximate nearest neighbors
- **Two-layer Chunking (Thematic "super-plus super" chunking)**: Local token-based chunks plus aggregated superchunks for global-to-local retrieval
- **Streaming Responses**: Real-time answer generation with GPT-4o-mini

### üìÑ **Document Processing**
- **Intelligent PDF Parsing**: Extract and clean text from complex financial documents
- **Adaptive Chunking**: Model-specific chunk sizing (BGE-M3: 1500 tokens, others: 500 tokens)
- **Context Preservation**: Smart overlap to maintain semantic continuity
- **Two-layer Chunking Explained**:
  - Stage 1: Token-based chunking (fine-grained local chunks).
  - Stage 2: Thematic aggregation: combine several adjacent/related chunks into superchunks (global context). This enables a global retrieval pass (superchunk search) followed by a local retrieval pass (chunk search) for refined source selection.
- **Multi-document Support**: Query across multiple uploaded documents

### üí° **User Experience**
- **Web Interface**: Clean, intuitive Streamlit dashboard
- **Source Attribution**: Every answer shows exact page references
- **Query History**: Track and reuse previous questions
- **Fallback Mode**: Works without API key using spaCy extraction
- **Sample Questions**: Pre-loaded financial accounting queries

### ‚ö° **Performance & Scalability**
- **HNSW (FAISS)**: Adopted HNSWIndex for scalable, low-latency approximate nearest neighbor search. Notes:
  - Normalize vectors (L2) before using inner-product or cosine with FAISS.
  - Use contiguous float32 arrays: np.ascontiguousarray(emb.astype("float32")).
  - Tune efConstruction (build-time) and efSearch (query-time) ‚Äî higher efSearch improves recall at cost of latency.
  - Maintain separate FAISS indexes for chunk-level and superchunk-level vectors to avoid rebuild/overwrite issues.
- **GPU Acceleration**: CUDA support for faster embedding generation
- **Batch Processing**: Optimized embedding generation with progress tracking
- **Memory Efficient**: Smart resource management for large documents
- **Cost Optimized**: ~$0.001 per query using GPT-4o-mini

---

## üèóÔ∏è **Architecture**

```mermaid
graph TB
    %% Upload and extraction
    A[PDF Upload] --> B[Text Extraction]

    %% Chunking layers
    B --> C[Token-based Chunking (fine-grained chunks)]
    C --> D[SuperChunk Aggregation (thematic / global)]

    %% Embeddings and store
    D --> E[Embedding Generation]
    E --> F[Vector Store - FAISS HNSW Indexes]

    %% Separate FAISS indexes
    F --> H[Superchunk HNSW Index]
    F --> I[Chunk HNSW Index]

    %% Query flow
    J[User Query] --> K[Query Rewriter]
    K --> L[Query Embedding]
    L --> H  %% global search on superchunks
    H --> M[Retrieve Top Superchunks]
    M --> I  %% refine by searching chunk index within selected superchunks
    I --> N[Context Retrieval]
    N --> O[Answer Generation]
    O --> P[Streaming Response]

    %% Embedding models
    subgraph Embedding_Models
        Q[BGE-M3]
        R[BGE-Large]
        S[E5-Large]
        T[Multilingual]
    end

    E --> Q
    E --> R
    E --> S
    E --> T

    O --> U[GPT-4o-mini]
```

Note: I removed inline "%%" comments after node definitions (they caused the GitHub Mermaid parser error). Use node labels for short descriptions or put Mermaid comments on their own lines starting with `%%`.

---

## üì¶ **Installation**

### Prerequisites
- Python 3.9 or higher
- 4GB+ RAM recommended
- Optional: CUDA-compatible GPU for acceleration

### Quick Start
```bash
# Clone the repository
git clone https://github.com/jayeshbankoti007/Financial_RAG.git
cd Financial_RAG

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_sm

# Run the application
streamlit run streamlit_app.py
```

### Docker Installation (Coming Soon)
```bash
docker build -t financial-rag .
docker run -p 8501:8501 financial-rag
```

---

## üéØ **Usage**

### 1. **Setup OpenAI API Key**
```bash
# Option 1: Environment variable
export OPENAI_API_KEY="your-api-key-here"

# Option 2: Enter in web interface
# Navigate to sidebar ‚Üí Configuration ‚Üí Enter API Key
```

### 2. **Upload Documents**
- Support formats: PDF (up to 200MB)
- Best with: Financial reports, accounting textbooks, regulatory documents
- Multiple documents: Query across all uploaded files

### 3. **Ask Questions**
```
Sample Questions:
‚Ä¢ "How is revenue recognized under GAAP?"
‚Ä¢ "What are the components of a cash flow statement?"
‚Ä¢ "Explain the difference between FIFO and LIFO inventory methods"
‚Ä¢ "What is the purpose of depreciation in accounting?"
```

### 4. **Review Results**
- **Answer**: AI-generated response with page references
- **Sources**: Top-ranked document chunks used
- **Relevance Scores**: Similarity scores for each source

---

## ‚öôÔ∏è **Configuration**

### Model Selection
```python
# Available embedding models
MODEL_OPTIONS = {
    "BGE M3 (Recommended)": "BAAI/bge-m3",              # 8K context, multilingual
    "BGE Large": "BAAI/bge-large-en-v1.5",              # High performance
    "E5 Large": "intfloat/e5-large-v2",                 # Strong alternative  
    "E5 Multilingual": "intfloat/multilingual-e5-large" # 100+ languages
}
```

### Chunking Strategy
```python
# Model-specific optimized chunking
MODEL_CHUNK_CONFIGS = {
    "BAAI/bge-m3": {"chunk_size": 1500, "overlap": 150},      # Leverage 8K context
    "BAAI/bge-large-en-v1.5": {"chunk_size": 400, "overlap": 50}, # Conservative
    "intfloat/e5-large-v2": {"chunk_size": 450, "overlap": 50}     # Balanced
}
```

### HNSW & FAISS Settings (recommended)
```python
HNSW_M = 32
HNSW_EF_CONSTRUCTION = 125
HNSW_EF_SEARCH = 200  # increase for better recall at query time
FAISS_METRIC = "inner_product"  # use after normalizing vectors (L2)
```

### Advanced Settings
```python
# Fine-tune in config.py
CHUNK_SIZE = 500           # Default chunk size in tokens (if not model-specific)
CHUNK_OVERLAP = 50         # Overlap between chunks
BATCH_SIZE = 32            # Embedding batch size
MAX_TOKENS = 800           # GPT response length
TEMPERATURE = 0.2          # Response randomness (0=deterministic)
```

---

## üè¢ **Project Structure**

```
financial-rag-system/
‚îú‚îÄ‚îÄ üìÑ streamlit_app.py          # Main web interface
‚îú‚îÄ‚îÄ üìÑ config.py                 # Configuration settings
‚îú‚îÄ‚îÄ üìÑ requirements.txt          # Dependencies
‚îú‚îÄ‚îÄ üìÑ README.md                 # This file
‚îÇ
‚îú‚îÄ‚îÄ üìÅ core/                     # Core RAG functionality
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ rag_system.py         # Main orchestrator ‚Äî build once: embeddings + HNSW indexes
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ pdf_processor.py      # PDF text extraction
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ text_chunker.py       # Token chunker + SuperChunk aggregator (two-layer)
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ embedding_manager.py  # Embedding generation (persist model, avoid reload per query)
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ vector_store.py       # FAISS HNSW vector operations (separate indexes for chunk & superchunk)
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ answer_generator.py   # GPT answer generation
‚îÇ
‚îú‚îÄ‚îÄ üìÅ models/                   # Data models
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ document_chunk.py     # DocumentChunk dataclass
‚îÇ
‚îî‚îÄ‚îÄ üìÅ utils/                    # Utilities
    ‚îî‚îÄ‚îÄ üìÑ helpers.py             # Common utilities
```

---

## üî¨ **Technical Deep Dive**

### Two-layer chunking (thematic "super-plus super" chunking)
- Purpose: Combine local fine-grained semantic retrieval with global thematic retrieval.
- Flow:
  1. Token-level split -> produce local chunks with overlap.
  2. Thematic aggregation -> group sequences of related chunks into superchunks (global summaries/contexts).
  3. Query flow uses superchunk search first to get global context, then restricts/refines with chunk-level search inside selected superchunks for precise sourcing and better attribution.
- Benefits: faster, focused retrieval; fewer chunk-level searches per query; improved accuracy for broad, thematic questions.

### HNSW (FAISS) details & best practices
- HNSW is approximate but fast for large collections; tune efSearch for recall.
- Always normalize vectors for cosine/inner-product similarity (faiss.normalize_L2).
- Keep indexes separate (chunk_index, superchunk_index) to avoid rebuilds and logical mixing.
- Build indexes once during ingestion/setup; avoid rebuilding on each query.
- If using multiprocessing or ThreadPools during embedding, persist models and avoid repeated load/unload to prevent semaphore leaks.

### Performance Benchmarks
- **Processing Speed**: ~100 pages/minute (varies with model & hardware)
- **Query Response**: <2 seconds average (with HNSW tuned and embeddings cached)
- **Memory Usage**: ~2GB for 1000-page document
- **Cost**: ~$0.001 per query (GPT-4o-mini)

---

## üß™ **Example Queries & Results**

<details>
<summary><strong>üìä "What are the main components of a balance sheet?"</strong></summary>

**Answer:**
> Based on the document, a balance sheet has three main components:
> 
> **Assets** (Page 45): Resources owned by the company, including current assets (cash, inventory, accounts receivable) and non-current assets (property, equipment, investments).
>
> **Liabilities** (Page 47): Obligations owed to others, divided into current liabilities (accounts payable, short-term debt) and long-term liabilities (mortgages, bonds).
>
> **Stockholders' Equity** (Page 48): The residual interest in assets after deducting liabilities, including contributed capital and retained earnings.

**Sources Used:** Pages 45-48 from "Financial Accounting Fundamentals.pdf"

</details>

---

## ü§ù **Contributing**

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md).

### Development Setup
```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/

# Format code
black .
flake8 .

# Type checking  
mypy core/
```

---

## üêõ **Known Issues & Limitations**

### Current Limitations
- **PDF Complexity**: Complex layouts may affect text extraction
- **Memory Usage**: Large documents (1000+ pages) require significant RAM
- **Language Support**: Optimized for English financial documents
- **API Dependency**: Best results require OpenAI API key

### Troubleshooting
```bash
# Common issues and solutions

# Issue: CUDA out of memory
# Solution: Set device to CPU in sidebar or reduce batch size

# Issue: Slow embedding generation / repeated model loads
# Solution: Persist SentenceTransformer (st.cache_resource or session_state) and avoid re-initializing on each query.

# Issue: HNSW poor recall or -1 indices returned
# Solution: Normalize vectors, ensure float32 contiguous arrays, and increase efSearch.
```

---

## üìÑ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè **Acknowledgments**

- **BAAI** for the excellent BGE embedding models
- **Microsoft** for E5 embedding models  
- **OpenAI** for GPT-4o-mini
- **Facebook** for FAISS vector search
- **Streamlit** for the amazing web framework
- **LangChain** for text processing utilities

---

## üìû **Support & Contact**

- **Issues**: [GitHub Issues](https://github.com/jayeshbankoti007/Financial_RAG/issues)
- **Discussions**: [GitHub Discussions](https://github.com/jayeshbankoti007/Financial_RAG/discussions)
- **Email**: jayeshbankoti@gmail.com
- **LinkedIn**: [Your Profile](https://linkedin.com/in/jayeshbankoti)
- **Website**: [Website](https://jayeshbankoti.site)

---

<div align="center">

**Made with ‚ù§Ô∏è for the Open Source Community**

If this project helps you, please consider giving it a ‚≠ê!

[‚¨Ü Back to Top](#-financial-rag-qa-system)

</div>